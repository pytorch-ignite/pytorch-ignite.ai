<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-NQNJSZEFB4"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-NQNJSZEFB4',{anonymize_ip:!1})}</script><meta property="og:title" content="Distributed Training Made Easy with PyTorch-Ignite"><meta property="og:description" content="Distributed code with PyTorch-Ignite"><meta property="og:type" content="article"><meta property="og:url" content="https://pytorch-ignite.ai/posts/distributed-made-easy-with-ignite/"><meta property="og:image" content="https://pytorch-ignite.ai/images/logos/ignite_logo_feature.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-06-28T00:00:00+00:00"><meta property="article:modified_time" content="2021-08-11T00:55:43+02:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://pytorch-ignite.ai/images/logos/ignite_logo_feature.png"><meta name=twitter:title content="Distributed Training Made Easy with PyTorch-Ignite"><meta name=twitter:description content="Distributed code with PyTorch-Ignite"><meta name=description content="High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently."><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#ee4c2c"><title>Distributed Training Made Easy with PyTorch-Ignite | PyTorch-Ignite</title><link rel=stylesheet type=text/css href=/css/style.min.125203f72a92a60311cb74c041a2ee40c9a6af792496be325e393ab46013c90b.css integrity="sha256-ElID9yqSpgMRy3TAQaLuQMmmr3kklr4yXjk6tGATyQs="><link rel=stylesheet type=text/css href=/css/icons.css><link rel=stylesheet href=/css/custom.min.c8f188c4d411d4bd166d8bb069a1cf75538ea63eb0acc05b8a7ceab44e934224.css><link rel=stylesheet href=/css/sponsors.min.273831ae344a358d21cb7ddb8405ef325fe44a83b83e4d37c971b03189b642b1.css><link rel=stylesheet href=/css/features.min.e0e90cfae6f294d73e9cd004efadaf0185e51575d39356cc5b06d4e24e8cf5f4.css><link rel=icon type=image/svg+xml href=/images/logos/ignite_logomark.svg><link rel=stylesheet href=https://rsms.me/inter/inter.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css><link rel=stylesheet href=/css/post.min.84329ebcd6e81746956e818bfd9674176663c4db7b7730d544587934955d3701.css></head><body><nav class="navbar is-fresh is-transparent no-shadow" role=navigation aria-label="main navigation"><div class=container><div class=navbar-brand><a class=navbar-item href=/><img src=/images/logos/ignite_logomark.svg alt width=112 height=28></a>
<a class="navbar-item is-hidden-desktop is-hidden-tablet"><div id=menu-icon-wrapper class=menu-icon-wrapper style=visibility:visible><svg width="1e3" height="1e3"><path class="path1" d="M3e2 4e2H7e2c2e2.0 2e2 350-1e2 450A4e2 4e2.0 012e2 2e2L8e2 8e2"/><path class="path2" d="M3e2 5e2H7e2"/><path class="path3" d="M7e2 6e2H3e2c-2e2.0-2e2-4e2 1e2-450A4e2 380 0 112e2 8e2L8e2 2e2"/></svg><button id=menu-icon-trigger class=menu-icon-trigger></button></div></a><a role=button class=navbar-burger aria-label=menu aria-expanded=false data-target=navbar-menu><span aria-hidden=true></span><span aria-hidden=true></span><span aria-hidden=true></span></a></div><div id=navbar-menu class="navbar-menu is-static"><div class=navbar-start><a class="navbar-item is-hidden-mobile"><div id=menu-icon-wrapper class=menu-icon-wrapper style=visibility:visible><svg width="1e3" height="1e3"><path class="path1" d="M3e2 4e2H7e2c2e2.0 2e2 350-1e2 450A4e2 4e2.0 012e2 2e2L8e2 8e2"/><path class="path2" d="M3e2 5e2H7e2"/><path class="path3" d="M7e2 6e2H3e2c-2e2.0-2e2-4e2 1e2-450A4e2 380 0 112e2 8e2L8e2 2e2"/></svg><button id=menu-icon-trigger class=menu-icon-trigger></button></div></a></div><div class=navbar-end><div class="navbar-item has-dropdown is-hoverable"><a class=navbar-link>Docs</a><div class=navbar-dropdown><a href=/tutorials/ class=navbar-item>Tutorials</a>
<a href=/how-to-guides/ class=navbar-item>How to Guides</a>
<a href=https://pytorch.org/ignite/engine.html class=navbar-item>API Reference</a></div></div><a href=https://github.com/pytorch/ignite class="navbar-item is-secondary">GitHub</a>
<a href=/posts/ class="navbar-item is-secondary">Blog</a>
<a href=/community/ class="navbar-item is-secondary">Community</a></div></div></div></nav><section class="section is-medium has-sidebar"><div class="container sidebar-on"><div class=columns><div class="column is-centered-tablet-portrait"><div class=date><span class=sr-only>Published on</span>
June 28, 2021</div><h1 class=post-title>Distributed Training Made Easy with PyTorch-Ignite</h1><div class=post-image><img src=/images/logos/ignite_logo_mixed.svg alt="PyTorch-Ignite logo"></div><h5 class="subtitle is-muted is-5"></h5><div class=divider></div><div class="mt-20 tags"><span class=tag><a href=/tags/deep-learning/>Deep Learning</a></span>
<span class=tag><a href=/tags/machine-learning/>Machine Learning</a></span>
<span class=tag><a href=/tags/pytorch-ignite/>PyTorch-Ignite</a></span>
<span class=tag><a href=/tags/pytorch/>PyTorch</a></span>
<span class=tag><a href=/tags/horovod/>Horovod</a></span>
<span class=tag><a href=/tags/slurm/>SLURM</a></span>
<span class=tag><a href=/tags/pytorch-xla/>PyTorch XLA</a></span>
<span class=tag><a href=/tags/pytorch-ddp/>PyTorch DDP</a></span>
<span class=tag><a href=/tags/distributed/>Distributed</a></span></div><div>Last updated: August 11, 2021</div></div></div><div class=content><p>Writing <a href=https://en.wikipedia.org/wiki/Agnostic_(data)>agnostic</a> <a href=https://pytorch.org/tutorials/beginner/dist_overview.html>distributed code</a> that supports different platforms, hardware configurations (GPUs, TPUs) and communication frameworks is tedious. In this blog, we will discuss how <a href=https://pytorch.org/ignite/>PyTorch-Ignite</a> solves this problem with minimal code change.</p><p>Crossposted from <a href=https://labs.quansight.org/blog/2021/06/distributed-made-easy-with-ignite/>https://labs.quansight.org/blog/2021/06/distributed-made-easy-with-ignite/</a></p><h2 id=prerequisites>Prerequisites</h2><p>This blog assumes you have some knowledge about:</p><ol><li><p><a href=https://pytorch.org/docs/stable/distributed.html#basics>PyTorch&rsquo;s distributed package</a>, the <a href=https://pytorch.org/docs/stable/distributed.html#backends>backends</a> and <a href=https://pytorch.org/docs/stable/distributed.html#collective-functions>collective functions</a> it provides. In this blog, we will focus on <a href=https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>distributed data parallel code</a>.</p></li><li><p><a href=https://pytorch-ignite.ai>PyTorch-Ignite</a>. Refer to this <a href=https://pytorch-ignite.ai/posts/introduction>blog</a> for a quick high-level overview.</p></li></ol><h2 id=introduction>Introduction</h2><p><a href=https://github.com/pytorch/ignite>PyTorch-Ignite&rsquo;s</a> <a href=https://pytorch.org/ignite/distributed.html>ignite.distributed</a> (<code>idist</code>) submodule introduced in version <a href=https://github.com/pytorch/ignite/releases/tag/v0.4.0.post1>v0.4.0 (July 2020)</a> quickly turns single-process code into its data distributed version.</p><p>Thus, you will now be able to run the same version of the code across all supported backends seamlessly:</p><ul><li><p>backends from native torch distributed configuration: <a href=https://github.com/NVIDIA/nccl>nccl</a>, <a href=https://github.com/facebookincubator/gloo>gloo</a>, <a href=https://www.open-mpi.org/>mpi</a>.</p></li><li><p><a href=https://horovod.readthedocs.io/en/stable/>Horovod</a> framework with <code>gloo</code> or <code>nccl</code> communication backend.</p></li><li><p>XLA on TPUs via <a href=https://github.com/pytorch/xla>pytorch/xla</a>.</p></li></ul><p>In this blog post we will compare PyTorch-Ignite&rsquo;s API with torch native&rsquo;s distributed code and highlight the differences and ease of use of the former. We will also show how Ignite&rsquo;s <code>auto_*</code> methods automatically make your code compatible with the aforementioned distributed backends so that you only have to bring your own model, optimizer and data loader objects.</p><p>Code snippets, as well as commands for running all the scripts, are provided in a separate <a href=https://github.com/pytorch-ignite/idist-snippets>repository</a>.</p><p>Then we will also cover several ways of spawning processes via torch native <code>torch.multiprocessing.spawn</code> and also via multiple distributed launchers in order to highlight how Pytorch-Ignite&rsquo;s <code>idist</code> can handle it without any changes to the code, in particular:</p><ul><li><p><a href=https://pytorch.org/docs/stable/multiprocessing.html#torch.multiprocessing.spawn><code>torch.multiprocessing.spawn</code></a></p></li><li><p><a href=https://pytorch.org/docs/stable/distributed.html#launch-utility><code>torch.distributed.launch</code></a></p></li><li><p><a href=https://horovod.readthedocs.io/en/stable/running_include.html><code>horovodrun</code></a></p></li><li><p><a href=https://slurm.schedmd.com/><code>slurm</code></a></p></li></ul><p>More information on launchers experiments can be found <a href=https://github.com/sdesrozis/why-ignite>here</a>.</p><h2 id=pytorch-ignite-unified-distributed-api>PyTorch-Ignite Unified Distributed API</h2><p>We need to write different code for different distributed backends. This can be tedious especially if you would like to run your code on different hardware configurations. Pytorch-Ignite&rsquo;s <code>idist</code> will do all the work for you, owing to the high-level helper methods.</p><h3 id=focus-on-the-helper-auto_-methods>Focus on the helper <code>auto_*</code> methods</h3><ul><li><a href=https://pytorch.org/ignite/distributed.html#ignite.distributed.auto.auto_model><code>auto_model()</code></a></li></ul><p>This method adapts the logic for non-distributed and available distributed configurations. Here are the equivalent code snippets for distributed model instantiation:</p><table><thead><tr><th style=text-align:center>PyTorch-Ignite</th><th style=text-align:center>PyTorch DDP</th></tr></thead></table><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/ignite_vs_ddp_automodel.png alt=ignite_vs_ddp_automodel width=100%><table><thead><tr><th style=text-align:center>Horovod</th><th style=text-align:center>Torch XLA</th></tr></thead></table><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/horovod_vs_xla_automodel.png alt=horovod_vs_xla_automodel width=100%><hr><p>Additionally, it is also compatible with <a href=https://github.com/NVIDIA/apex>NVIDIA/apex</a></p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>model, optimizer <span style=color:#f92672>=</span> amp<span style=color:#f92672>.</span>initialize(model, optimizer, opt_level<span style=color:#f92672>=</span>opt_level)
model <span style=color:#f92672>=</span> idist<span style=color:#f92672>.</span>auto_model(model)
</code></pre></td></tr></table></div></div><p>and <a href=https://pytorch.org/docs/stable/amp.html>Torch native AMP</a></p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>model <span style=color:#f92672>=</span> idist<span style=color:#f92672>.</span>auto_model(model)

<span style=color:#66d9ef>with</span> autocast():
    y_pred <span style=color:#f92672>=</span> model(x)
</code></pre></td></tr></table></div></div><ul><li><a href=https://pytorch.org/ignite/distributed.html#ignite.distributed.auto.auto_model><code>auto_optim()</code></a></li></ul><p>This method adapts the optimizer logic for non-distributed and available distributed configurations seamlessly. Here are the equivalent code snippets for distributed optimizer instantiation:</p><table><thead><tr><th style=text-align:center>PyTorch-Ignite</th><th style=text-align:center>PyTorch DDP</th></tr></thead></table><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/ignite_vs_ddp_autooptim.png alt=ignite_vs_ddp_autooptim width=100%><table><thead><tr><th style=text-align:center>Horovod</th><th style=text-align:center>Torch XLA</th></tr></thead></table><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/horovod_vs_xla_autooptim.png alt=horovod_vs_xla_autooptim width=100%><hr><ul><li><a href=https://pytorch.org/ignite/distributed.html#ignite.distributed.auto.auto_dataloader><code>auto_dataloader()</code></a></li></ul><p>This method adapts the data loading logic for non-distributed and available distributed configurations seamlessly on target devices.</p><p>Additionally, <code>auto_dataloader()</code> automatically scales the batch size according to the distributed configuration context resulting in a general way of loading sample batches on multiple devices.</p><p>Here are the equivalent code snippets for the distributed data loading step:</p><table><thead><tr><th style=text-align:center>PyTorch-Ignite</th><th style=text-align:center>PyTorch DDP</th></tr></thead></table><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/ignite_vs_ddp_autodataloader.png alt=ignite_vs_ddp_autodataloader width=100%><table><thead><tr><th style=text-align:center>Horovod</th><th style=text-align:center>Torch XLA</th></tr></thead></table><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/ignite_vs_ddp_autodataloader.png alt=ignite_vs_ddp_autodataloader width=100%><hr><div class="admonition tip"><p class=admonition-title>NOTE</p><p>Additionally, <code>idist</code> provides collective operations like <code>all_reduce</code>, <code>all_gather</code>, and <code>broadcast</code> that can be used with all supported distributed frameworks. Please, see <a href=https://pytorch.org/ignite/distributed.html#ignite-distributed-utils>our documentation</a> for more details.</p></div><h2 id=examples>Examples</h2><p>The code snippets below highlight the API&rsquo;s specificities of each of the distributed backends on the same use case as compared to the <code>idist</code> API. PyTorch native code is available for DDP, Horovod, and for XLA/TPU devices.</p><p>PyTorch-Ignite&rsquo;s unified code snippet can be run with the standard PyTorch backends like <code>gloo</code> and <code>nccl</code> and also with Horovod and XLA for TPU devices. Note that the code is less verbose, however, the user still has full control of the training loop.</p><p>The following examples are introductory. For a more robust, production-grade example that uses PyTorch-Ignite, refer <a href=https://github.com/pytorch/ignite/tree/master/examples/contrib/cifar10>here</a>.</p><p>The complete source code of these experiments can be found <a href=https://github.com/pytorch-ignite/idist-snippets>here</a>.</p><h3 id=pytorch-ignite---torch-native-distributed-data-parallel---horovod---xlatpus>PyTorch-Ignite - Torch native Distributed Data Parallel - Horovod - XLA/TPUs</h3><table><thead><tr><th style=text-align:center>PyTorch-Ignite</th><th style=text-align:center>PyTorch DDP</th></tr></thead><tbody><tr><td style=text-align:center><a href=https://github.com/pytorch-ignite/idist-snippets/blob/master/ignite_idist.py>Source Code</a></td><td style=text-align:center><a href=https://github.com/pytorch-ignite/idist-snippets/blob/master/torch_native.py>Source Code</a></td></tr></tbody></table><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/ignite_vs_ddp_whole.png alt=ignite_vs_ddp_whole width=100%><table><thead><tr><th style=text-align:center>Horovod</th><th style=text-align:center>Torch XLA</th></tr></thead><tbody><tr><td style=text-align:center><a href=https://github.com/pytorch-ignite/idist-snippets/blob/master/torch_horovod.py>Source Code</a></td><td style=text-align:center><a href=https://github.com/pytorch-ignite/idist-snippets/blob/master/torch_xla_native.py>Source Code</a></td></tr><tr><td style=text-align:center><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/horovod_whole.png alt=horovod_whole></td><td style=text-align:center><img src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/images/pytorch-ignite/distributed-made-easy-with-ignite/xla_whole.png alt=xla_whole></td></tr></tbody></table><hr><div class="admonition tip"><p class=admonition-title>NOTE</p><p><p>You can also mix the usage of <code>idist</code> with other distributed APIs as below:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py>dist<span style=color:#f92672>.</span>init_process_group(backend, store<span style=color:#f92672>=...</span>, world_size<span style=color:#f92672>=</span>world_size, rank<span style=color:#f92672>=</span>rank)
<p>rank <span style=color:#f92672>=</span> idist<span style=color:#f92672>.</span>get_rank()
ws <span style=color:#f92672>=</span> idist<span style=color:#f92672>.</span>get_world_size()
model <span style=color:#f92672>=</span> idist<span style=color:#f92672>.</span>auto_model(model)
dist<span style=color:#f92672>.</span>destroy_process_group()
</code></pre></td></tr></table></p></div></div></p></div><h2 id=running-distributed-code>Running Distributed Code</h2><p>PyTorch-Ignite&rsquo;s <code>idist</code> also unifies the distributed codes launching method and makes the distributed configuration setup easier with the
<a href=https://pytorch.org/ignite/distributed.html#ignite.distributed.launcher.Parallel>ignite.distributed.launcher.Parallel (idist Parallel)</a> context manager.</p><p>This context manager has the capability to either spawn <code>nproc_per_node</code> (passed as a script argument) child processes and initialize a processing group according to the provided backend or use tools like <code>torch.distributed.launch</code>, <code>slurm</code>, <code>horovodrun</code> by initializing the processing group given the <code>backend</code> argument only
in a general way.</p><h3 id=with-torchmultiprocessingspawn>With <code>torch.multiprocessing.spawn</code></h3><p>In this case <code>idist Parallel</code> is using the native torch <code>torch.multiprocessing.spawn</code> method under the hood in order to run
the distributed configuration. Here <code>nproc_per_node</code> is passed as a spawn argument.</p><ul><li>Running multiple distributed configurations with one code. Source: <a href=https://github.com/pytorch-ignite/idist-snippets/blob/master/ignite_idist.py>ignite_idist.py</a>:</li></ul><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e># Running with gloo</span>
python -u ignite_idist.py --nproc_per_node <span style=color:#ae81ff>2</span> --backend gloo

<span style=color:#75715e># Running with nccl</span>
python -u ignite_idist.py --nproc_per_node <span style=color:#ae81ff>2</span> --backend nccl

<span style=color:#75715e># Running with horovod with gloo controller ( gloo or nccl support )</span>
python -u ignite_idist.py --backend horovod --nproc_per_node <span style=color:#ae81ff>2</span>

<span style=color:#75715e># Running on xla/tpu</span>
python -u ignite_idist.py --backend xla-tpu --nproc_per_node <span style=color:#ae81ff>8</span> --batch_size <span style=color:#ae81ff>32</span>
</code></pre></td></tr></table></div></div><h3 id=with-distributed-launchers>With Distributed launchers</h3><p>PyTorch-Ignite&rsquo;s <code>idist Parallel</code> context manager is also compatible
with multiple distributed launchers.</p><h4 id=with-torchdistributedlaunch>With torch.distributed.launch</h4><p>Here we are using the <code>torch.distributed.launch</code> script in order to
spawn the processes:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>python -m torch.distributed.launch --nproc_per_node <span style=color:#ae81ff>2</span> --use_env ignite_idist.py --backend gloo
</code></pre></td></tr></table></div></div><h4 id=with-horovodrun>With horovodrun</h4><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>horovodrun -np <span style=color:#ae81ff>4</span> -H hostname1:2,hostname2:2 python ignite_idist.py --backend horovod
</code></pre></td></tr></table></div></div><div class="admonition tip"><p class=admonition-title>NOTE</p><p><p>In order to run this example and to avoid the installation procedure, you can pull one of PyTorch-Ignite&rsquo;s <a href=https://github.com/pytorch/ignite/blob/master/docker/hvd/Dockerfile.hvd-base>docker image with pre-installed Horovod</a>. It will include Horovod with <code>gloo</code> controller and <code>nccl</code> support.</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>docker run --gpus all -it -v $PWD:/project pytorchignite/hvd-vision:latest /bin/bash
cd project
</code></pre></td></tr></table></div></div></p></div><h4 id=with-slurm>With slurm</h4><p>The same result can be achieved by using <code>slurm</code> without any
modification to the code:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">5
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">6
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">7
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">8
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>srun --nodes<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>
    --ntasks-per-node<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>
    --job-name<span style=color:#f92672>=</span>pytorch-ignite
    --time<span style=color:#f92672>=</span>00:01:00
    --partition<span style=color:#f92672>=</span>gpgpu
    --gres<span style=color:#f92672>=</span>gpu:2
    --mem<span style=color:#f92672>=</span>10G
    python ignite_idist.py --backend nccl
</code></pre></td></tr></table></div></div><p>or using <code>sbatch script.bash</code> with the script file <code>script.bash</code>:</p><div class=highlight><div style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><table style=border-spacing:0;padding:0;margin:0;border:0;width:auto;overflow:auto;display:block><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span><span style=color:#75715e>#SBATCH --job-name=pytorch-ignite</span>
<span style=color:#75715e>#SBATCH --output=slurm_%j.out</span>
<span style=color:#75715e>#SBATCH --nodes=2</span>
<span style=color:#75715e>#SBATCH --ntasks-per-node=2</span>
<span style=color:#75715e>#SBATCH --time=00:01:00</span>
<span style=color:#75715e>#SBATCH --partition=gpgpu</span>
<span style=color:#75715e>#SBATCH --gres=gpu:2</span>
<span style=color:#75715e>#SBATCH --mem=10G</span>

srun python ignite_idist.py --backend nccl
</code></pre></td></tr></table></div></div><h2 id=closing-remarks>Closing Remarks</h2><p>As we saw through the above examples, managing multiple configurations
and specifications for distributed computing has never been easier. In
just a few lines we can parallelize and execute code wherever it is
while maintaining control and simplicity.</p><h2 id=references>References</h2><ul><li><p><a href=https://github.com/pytorch-ignite/idist-snippets/>idist-snippets</a>:
complete code used in this post.</p></li><li><p><a href=https://github.com/sdesrozis/why-ignite>why-ignite</a>: examples
with distributed data parallel: native pytorch, pytorch-ignite,
slurm.</p></li><li><p><a href=https://github.com/pytorch/ignite/tree/master/examples/contrib/cifar10>CIFAR10 example</a>
of distributed training on CIFAR10 with muliple configurations: 1 or
multiple GPUs, multiple nodes and GPUs, TPUs.</p></li></ul></div><div class=prev-n-next><div class=prev><a href=/posts/introduction/ class=link><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify icon-prev iconify--carbon" width="32" height="32" viewBox="0 0 32 32"><path d="M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6 4 16l10 10z" fill="currentcolor"/></svg><span class=text>Introduction to PyTorch-Ignite</span></a></div><div class=next><a href=/posts/introducing-code-generator-v020/ class=link><span class=text>Introducing PyTorch-Ignite's Code Generator v0.2.0</span><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify icon-next iconify--carbon" width="32" height="32" viewBox="0 0 32 32"><path d="M18 6l-1.43 1.393L24.15 15H4v2h20.15l-7.58 7.573L18 26l10-10L18 6z" fill="currentcolor"/></svg></a></div></div></div></section><div id=backtotop><a href=#></a></div><div class=sidebar><div class=sidebar-header><div class=text>Posts</div></div><div class=inner><aside><ul><li><a href=/posts/gan-evaluation-with-fid-and-is/>GAN evaluation using FID and IS</a></li><li><a href=/posts/introducing-code-generator-v020/>Introducing PyTorch-Ignite's Code Generator v0.2.0</a></li><li><a href=/posts/distributed-made-easy-with-ignite/ class=active>Distributed Training Made Easy with PyTorch-Ignite</a><nav id=TableOfContents><ul><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#pytorch-ignite-unified-distributed-api>PyTorch-Ignite Unified Distributed API</a><ul><li><a href=#focus-on-the-helper-auto_-methods>Focus on the helper <code>auto_*</code> methods</a></li></ul></li><li><a href=#examples>Examples</a><ul><li><a href=#pytorch-ignite---torch-native-distributed-data-parallel---horovod---xlatpus>PyTorch-Ignite - Torch native Distributed Data Parallel - Horovod - XLA/TPUs</a></li></ul></li><li><a href=#running-distributed-code>Running Distributed Code</a><ul><li><a href=#with-torchmultiprocessingspawn>With <code>torch.multiprocessing.spawn</code></a></li><li><a href=#with-distributed-launchers>With Distributed launchers</a></li></ul></li><li><a href=#closing-remarks>Closing Remarks</a></li><li><a href=#references>References</a></li></ul></nav></li><li><a href=/posts/introduction/>Introduction to PyTorch-Ignite</a></li></ul></aside></div></div><script src=https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/feather-icons@4.28.0/dist/feather.min.js></script><script src=/js/modernizr.min.48806575a40ede5e406c8278f3be521afb171e73d324994ced6b41306bdb230a.js></script><script src=/js/fresh.min.29a18383a93b919d639d60aca2437a08bec3ed15276e55dc1cb44a8ddd80bdc6.js></script><script src=/js/active-navbar.min.8067cab2f01a301954f5d28b2cb8cfa1ce10a23ea212f1c734c9b761503d5e0e.js></script><script src=/js/jquery.panelslider.min.js></script><link rel=stylesheet href=/css/copy-code.min.0032417fb3ddf66588bd7871ff6904ecb06695ccb2756f43792bbeec4b7da54f.css><script src=/js/copy-code.min.2597230d83cf7b0549c4a0d22b4e61a1b3fd86eaf05c491aa0b807c7803d9096.js></script><script src=/js/anchor.min.818717ad1ac9e378368b336d69c4a337a826937be38314c67bfca3669727e210.js></script></body></html>