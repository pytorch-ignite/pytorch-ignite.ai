<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>all_gather on PyTorch-Ignite</title><link>https://pytorch-ignite.ai/tags/all_gather/</link><description>Recent content in all_gather on PyTorch-Ignite</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://pytorch-ignite.ai/tags/all_gather/index.xml" rel="self" type="application/rss+xml"/><item><title>Collective Communication with Ignite</title><link>https://pytorch-ignite.ai/tutorials/advanced/01-collective-communication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://pytorch-ignite.ai/tutorials/advanced/01-collective-communication/</guid><description>&lt;h1 id="collective-communication-with-ignite">
&lt;a href="#collective-communication-with-ignite" class="header-anchor">
Collective Communication with Ignite
&lt;/a>
&lt;/h1>&lt;p>In this tutorial, we will see how to use advanced distributed functions like &lt;code>all_reduce()&lt;/code>, &lt;code>all_gather()&lt;/code>, &lt;code>broadcast()&lt;/code> and &lt;code>barrier()&lt;/code>. We will discuss unique use cases for all of them and represent them visually.&lt;/p></description></item></channel></rss>