<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>TPUs on Colab on PyTorch-Ignite</title><link>https://pytorch-ignite.ai/tags/tpus-on-colab/</link><description>Recent content in TPUs on Colab on PyTorch-Ignite</description><generator>Hugo -- gohugo.io</generator><lastBuildDate>Sat, 18 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://pytorch-ignite.ai/tags/tpus-on-colab/index.xml" rel="self" type="application/rss+xml"/><item><title>Distributed Training on CPUs, GPUs or TPUs</title><link>https://pytorch-ignite.ai/tutorials/intermediate/01-cifar10-distributed/</link><pubDate>Sat, 18 Sep 2021 00:00:00 +0000</pubDate><guid>https://pytorch-ignite.ai/tutorials/intermediate/01-cifar10-distributed/</guid><description>&lt;h1 id="distributed-training-with-ignite-on-cifar10">
&lt;a href="#distributed-training-with-ignite-on-cifar10" class="header-anchor">
Distributed Training with Ignite on CIFAR10
&lt;/a>
&lt;/h1>&lt;p>This tutorial is a brief introduction on how you can do distributed training with Ignite on one or more CPUs, GPUs or TPUs. We will also introduce several helper functions and Ignite concepts (setup common training handlers, save to/ load from checkpoints, etc.) which you can easily incorporate in your code.&lt;/p></description></item></channel></rss>